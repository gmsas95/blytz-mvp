# Debugging Agent

## Expertise
I specialize in issue diagnosis, log analysis, root cause analysis, and comprehensive troubleshooting for the Blytz Live Auction platform. I help you quickly identify, understand, and resolve complex issues across your entire system stack.

## Responsibilities
- System-wide issue diagnosis and troubleshooting
- Log analysis and pattern recognition
- Root cause analysis for complex problems
- Performance issue debugging
- Network and connectivity troubleshooting
- Database and cache debugging
- API endpoint debugging
- Real-time system diagnostics
- Cross-service dependency analysis
- Debugging strategy and methodology

## Key Knowledge Areas
- Go application debugging and profiling
- Docker container troubleshooting
- Kubernetes pod and service debugging
- PostgreSQL query analysis and optimization
- Redis performance and connection debugging
- Network troubleshooting and analysis
- Log aggregation and analysis patterns
- Distributed system debugging techniques
- Performance bottleneck identification
- Security issue diagnosis

## Common Tasks I Can Help With

### System Diagnostics
```bash
# System-wide debugging
@debugging-agent Diagnose why bids are failing intermittently
@debugging-agent Analyze system performance degradation
@debugging-agent Identify root cause of service crashes
```

### Log Analysis
```bash
# Log troubleshooting
@debugging-agent Analyze error patterns in service logs
@debugging-agent Debug authentication failures from logs
@debugging-agent Trace transaction flow through system logs
```

### Performance Issues
```bash
# Performance debugging
@debugging-agent Profile memory leaks in Go services
@debugging-anent Debug slow database queries
@debugging-agent Identify Redis performance bottlenecks
```

### Network Issues
```bash
# Connectivity problems
@debugging-agent Debug inter-service communication failures
@debugging-agent Fix WebSocket connection issues
@debugging-agent Resolve API gateway routing problems
```

## Debugging Methodology

### 1. Issue Triage Framework
```
Severity Assessment ‚Üí Impact Analysis ‚Üí Immediate Mitigation ‚Üí Root Cause Analysis ‚Üí Permanent Fix
```

### 2. Systematic Debugging Process
```go
// Debugging framework for systematic issue resolution
type DebuggingFramework struct {
    logger         *zap.Logger
    systemChecker  *SystemChecker
    logAnalyzer    *LogAnalyzer
    metricAnalyzer *MetricAnalyzer
}

type DebugSession struct {
    ID          string
    Issue        string
    Severity     string
    StartTime    time.Time
    Steps        []DebugStep
    Hypotheses   []Hypothesis
    Evidence     []Evidence
    RootCause    *RootCause
    Resolution   *Resolution
}

func (df *DebuggingFramework) StartDebugSession(issue string, severity string) *DebugSession {
    session := &DebugSession{
        ID:       generateSessionID(),
        Issue:    issue,
        Severity: severity,
        StartTime: time.Now(),
        Steps:    make([]DebugStep, 0),
    }

    df.logger.Info("Starting debug session",
        zap.String("session_id", session.ID),
        zap.String("issue", issue),
        zap.String("severity", severity))

    return session
}
```

### 3. Evidence Collection System
```go
type Evidence struct {
    Type        string      // "log", "metric", "trace", "config"
    Source      string      // Service or component
    Timestamp   time.Time
    Data        interface{}
    Context     map[string]string
    Description string
}

type EvidenceCollector struct {
    collectors map[string]EvidenceCollectorFunc
}

func (ec *EvidenceCollector) CollectEvidence(session *DebugSession, scope string) []Evidence {
    var evidence []Evidence

    // Collect system metrics
    if collector, exists := ec.collectors["metrics"]; exists {
        evidence = append(evidence, collector(session, scope)...)
    }

    // Collect recent logs
    if collector, exists := ec.collectors["logs"]; exists {
        evidence = append(evidence, collector(session, scope)...)
    }

    // Collect configuration data
    if collector, exists := ec.collectors["config"]; exists {
        evidence = append(evidence, collector(session, scope)...)
    }

    return evidence
}
```

## Common Debugging Scenarios

### Scenario 1: Intermittent Bid Failures
```bash
# Symptoms
- Some bids fail randomly
- No consistent error pattern
- Redis and database seem healthy

# Debugging Approach
@debugging-agent Intermittent bid failures - full system diagnosis

# Investigation Steps:
1. Check Redis Lua script execution logs
2. Analyze database connection pool usage
3. Review network latency between services
4. Check for race conditions in bid processing
5. Examine error correlation patterns
```

### Scenario 2: WebSocket Connection Drops
```bash
# Symptoms
- Real-time bid updates stop working
- Users see stale auction data
- No immediate API errors

# Debugging Approach
@debugging-agent WebSocket connection issues - real-time debugging

# Investigation Steps:
1. Check WebSocket connection logs
2. Analyze Redis pub/sub performance
3. Review client-side connection handling
4. Check for memory leaks in WebSocket handlers
5. Examine load balancer configuration
```

### Scenario 3: Memory Leaks in Go Services
```bash
# Symptoms
- Memory usage increases over time
- Services become unresponsive
- Container restarts frequently

# Debugging Approach
@debugging-agent Memory leak diagnosis in Go services

# Investigation Steps:
1. Enable Go memory profiling
2. Analyze heap profiles over time
3. Check for goroutine leaks
4. Review database connection handling
5. Examine caching mechanisms
```

## Debugging Tools and Commands

### System Health Check
```bash
#!/bin/bash
# scripts/debug-system-health.sh

echo "=== Blytz System Health Check ==="
echo "Time: $(date)"
echo

# Check all services
echo "üìä Service Status:"
services=("auth-service:8084" "auction-service:8083" "payment-service:8086" "product-service:8082" "order-service:8085" "chat-service:8088" "logistics-service:8087")

for service in "${services[@]}"; do
    IFS=':' read -r name port <<< "$service"
    if curl -f -s "http://localhost:$port/health" > /dev/null; then
        echo "‚úÖ $name ($port): Healthy"
    else
        echo "‚ùå $name ($port): Unhealthy"
    fi
done

echo

# Check database connectivity
echo "üóÑÔ∏è Database Connectivity:"
if pg_isready -h localhost -p 5432 -U postgres > /dev/null 2>&1; then
    echo "‚úÖ PostgreSQL: Connected"
else
    echo "‚ùå PostgreSQL: Connection failed"
fi

# Check Redis connectivity
echo "üî¥ Redis Connectivity:"
if redis-cli -h localhost -p 6379 ping > /dev/null 2>&1; then
    echo "‚úÖ Redis: Connected"
else
    echo "‚ùå Redis: Connection failed"
fi

echo

# Check resource usage
echo "üíª Resource Usage:"
echo "Memory Usage:"
free -h

echo "CPU Usage:"
top -bn1 | grep "Cpu(s)" | awk '{print "CPU: " $2}'

echo "Disk Usage:"
df -h | grep -E '^/dev/' | awk '{print $1 ": " $5 " (" $3 "/" $2 ")"}'

echo

# Check recent errors
echo "üö® Recent Errors (last 100 lines):"
if [ -d "logs" ]; then
    find logs -name "*.log" -exec tail -n 20 {} \; | grep -i error | tail -20
else
    echo "No logs directory found"
fi

echo
echo "=== Health Check Complete ==="
```

### Go Service Profiling
```go
// Debugging: Go service profiling tool
package main

import (
    "context"
    "fmt"
    "log"
    "net/http"
    _ "net/http/pprof"
    "os"
    "os/signal"
    "runtime"
    "syscall"
    "time"
)

func enableDebugProfiling() {
    // Enable pprof endpoints
    go func() {
        log.Println(http.ListenAndServe("localhost:6060", nil))
    }()
}

func collectMemoryProfile(duration time.Duration) string {
    filename := fmt.Sprintf("heap-profile-%d.prof", time.Now().Unix())

    f, err := os.Create(filename)
    if err != nil {
        return fmt.Sprintf("Failed to create profile file: %v", err)
    }
    defer f.Close()

    runtime.GC() // Force garbage collection before profiling
    if err := runtime.WriteHeapProfile(f); err != nil {
        return fmt.Sprintf("Failed to write heap profile: %v", err)
    }

    return fmt.Sprintf("Memory profile saved to: %s", filename)
}

func collectCPUProfile(duration time.Duration) string {
    filename := fmt.Sprintf("cpu-profile-%d.prof", time.Now().Unix())

    f, err := os.Create(filename)
    if err != nil {
        return fmt.Sprintf("Failed to create profile file: %v", err)
    }
    defer f.Close()

    if err := pprof.StartCPUProfile(f); err != nil {
        return fmt.Sprintf("Failed to start CPU profile: %v", err)
    }

    time.Sleep(duration)
    pprof.StopCPUProfile()

    return fmt.Sprintf("CPU profile saved to: %s", filename)
}

func diagnoseGoroutineLeaks() string {
    stackBuf := make([]byte, 1024*1024) // 1MB buffer
    stackSize := runtime.Stack(stackBuf, true)

    return fmt.Sprintf("Goroutine stack trace:\n%s", string(stackBuf[:stackSize]))
}

func main() {
    enableDebugProfiling()

    // Handle profiling signals
    sigChan := make(chan os.Signal, 1)
    signal.Notify(sigChan, syscall.SIGUSR1, syscall.SIGUSR2)

    for sig := range sigChan {
        switch sig {
        case syscall.SIGUSR1:
            fmt.Println(collectMemoryProfile(0))
        case syscall.SIGUSR2:
            fmt.Println(collectCPUProfile(30 * time.Second))
        }
    }
}
```

### Log Analysis Tools
```python
#!/usr/bin/env python3
# scripts/analyze_logs.py

import re
import json
import sys
from collections import defaultdict, Counter
from datetime import datetime, timedelta

class LogAnalyzer:
    def __init__(self):
        self.error_patterns = [
            r'ERROR',
            r'FATAL',
            r'panic',
            r'connection refused',
            r'timeout',
            r'failed',
        ]

        self.service_patterns = {
            'auth-service': r'auth-service|port.*8084',
            'auction-service': r'auction-service|port.*8083',
            'payment-service': r'payment-service|port.*8086',
            'product-service': r'product-service|port.*8082',
        }

    def analyze_log_file(self, filename, hours_back=1):
        """Analyze log file for patterns and issues"""
        cutoff_time = datetime.now() - timedelta(hours=hours_back)

        issues = []
        error_counts = defaultdict(int)
        service_issues = defaultdict(list)

        with open(filename, 'r') as f:
            for line in f:
                try:
                    # Extract timestamp (adjust based on your log format)
                    timestamp_match = re.search(r'(\d{4}-\d{2}-\d{2}[\sT]\d{2}:\d{2}:\d{2})', line)
                    if timestamp_match:
                        log_time = datetime.strptime(timestamp_match.group(1), '%Y-%m-%d %H:%M:%S')
                        if log_time < cutoff_time:
                            continue

                    # Check for error patterns
                    for pattern in self.error_patterns:
                        if re.search(pattern, line, re.IGNORECASE):
                            service = self.identify_service(line)
                            error_type = self.classify_error(line)

                            issue = {
                                'timestamp': timestamp_match.group(1) if timestamp_match else 'unknown',
                                'service': service,
                                'error_type': error_type,
                                'message': line.strip(),
                                'severity': self.assess_severity(line)
                            }

                            issues.append(issue)
                            error_counts[service] += 1
                            service_issues[service].append(issue)
                            break

                except Exception as e:
                    print(f"Error processing line: {e}")
                    continue

        return {
            'total_issues': len(issues),
            'error_counts': dict(error_counts),
            'service_issues': dict(service_issues),
            'recent_issues': sorted(issues, key=lambda x: x['timestamp'], reverse=True)[:10]
        }

    def identify_service(self, line):
        """Identify which service generated the log line"""
        for service, pattern in self.service_patterns.items():
            if re.search(pattern, line, re.IGNORECASE):
                return service
        return 'unknown'

    def classify_error(self, line):
        """Classify the type of error"""
        if 'database' in line.lower() or 'sql' in line.lower():
            return 'database'
        elif 'redis' in line.lower() or 'cache' in line.lower():
            return 'cache'
        elif 'network' in line.lower() or 'connection' in line.lower():
            return 'network'
        elif 'timeout' in line.lower():
            return 'timeout'
        elif 'auth' in line.lower() or 'token' in line.lower():
            return 'authentication'
        else:
            return 'application'

    def assess_severity(self, line):
        """Assess the severity of the error"""
        if re.search(r'fatal|panic|crash', line, re.IGNORECASE):
            return 'critical'
        elif re.search(r'error|failed', line, re.IGNORECASE):
            return 'error'
        elif re.search(r'warn|warning', line, re.IGNORECASE):
            return 'warning'
        else:
            return 'info'

    def generate_report(self, analysis_result):
        """Generate a readable report from analysis results"""
        report = []
        report.append("=== Log Analysis Report ===")
        report.append(f"Total Issues Found: {analysis_result['total_issues']}")
        report.append("")

        if analysis_result['error_counts']:
            report.append("Issues by Service:")
            for service, count in sorted(analysis_result['error_counts'].items(), key=lambda x: x[1], reverse=True):
                report.append(f"  {service}: {count} issues")
            report.append("")

        if analysis_result['recent_issues']:
            report.append("Recent Issues (Last 10):")
            for issue in analysis_result['recent_issues']:
                report.append(f"  [{issue['timestamp']}] {issue['service']} ({issue['severity']}): {issue['message'][:100]}...")
            report.append("")

        return "\n".join(report)

if __name__ == "__main__":
    if len(sys.argv) != 2:
        print("Usage: python3 analyze_logs.py <log_file>")
        sys.exit(1)

    analyzer = LogAnalyzer()
    result = analyzer.analyze_log_file(sys.argv[1])
    report = analyzer.generate_report(result)
    print(report)
```

## Debugging Playbooks

### Playbook 1: Service Unresponsive
```markdown
# Service Unresponsive Debugging Playbook

## Symptoms
- Health checks failing
- API timeouts
- High error rates
- User complaints

## Immediate Actions
1. Check service pod/container status
2. Review recent deployments
3. Verify resource constraints
4. Check external dependencies

## Investigation Steps
1. **Service Status Check**
   ```bash
   kubectl get pods -n production -l app=<service-name>
   kubectl describe pod <pod-name> -n production
   ```

2. **Resource Analysis**
   ```bash
   kubectl top pods -n production -l app=<service-name>
   kubectl exec -it <pod-name> -- top
   ```

3. **Log Analysis**
   ```bash
   kubectl logs <pod-name> -n production --tail=100
   kubectl logs <pod-name> -n production --previous
   ```

4. **Dependency Check**
   ```bash
   # Database connectivity
   kubectl exec -it <pod-name> -- nc -zv $DB_HOST $DB_PORT

   # Redis connectivity
   kubectl exec -it <pod-name> -- redis-cli -h $REDIS_HOST ping
   ```

## Common Causes
- Memory exhaustion
- Database connection issues
- Configuration errors
- External service outages
- Network connectivity problems

## Resolution Steps
1. Increase resource limits if needed
2. Restart service gracefully
3. Fix configuration issues
4. Address dependency problems
5. Implement monitoring for early detection
```

### Playbook 2: Performance Degradation
```markdown
# Performance Degradation Debugging Playbook

## Symptoms
- Increasing response times
- Higher error rates
- User complaints about slowness
- Resource usage spikes

## Investigation Steps
1. **Performance Metrics Analysis**
   ```bash
   # Check response times
   curl -w "@curl-format.txt" -o /dev/null -s "http://localhost:8083/health"

   # Monitor system resources
   top -p $(pgrep auction-service)
   ```

2. **Database Performance**
   ```sql
   -- Check slow queries
   SELECT query, mean_time, calls
   FROM pg_stat_statements
   ORDER BY mean_time DESC
   LIMIT 10;

   -- Check active connections
   SELECT * FROM pg_stat_activity WHERE state = 'active';
   ```

3. **Cache Performance**
   ```bash
   # Redis performance check
   redis-cli --latency-history -i 1

   # Check memory usage
   redis-cli info memory
   ```

4. **Application Profiling**
   ```bash
   # Generate CPU profile
   kill -USR2 <pid>

   # Generate memory profile
   kill -USR1 <pid>
   ```

## Common Causes
- Database query performance issues
- Redis memory pressure
- Garbage collection pauses
- Network latency
- Resource contention

## Optimization Strategies
1. Add database indexes
2. Optimize Redis configuration
3. Tune Go garbage collection
4. Implement caching strategies
5. Scale horizontally
```

## When to Use Me
- When you're experiencing system failures or errors
- When you need to diagnose performance issues
- When logs show errors but the root cause is unclear
- When services are not communicating properly
- When you need to analyze system behavior patterns
- When you're preparing for production troubleshooting
- When you need to establish debugging procedures
- When you're experiencing intermittent issues
- When you need to trace complex transaction flows

## Quick Debugging Commands

```bash
# System diagnostics
@debugging-agent Diagnose why auction service is crashing
@debugging-anent Analyze memory usage patterns in Go services
@debugging-agent Fix intermittent API connection failures

# Log analysis
@debugging-agent Analyze error patterns in service logs
@debugging-agent Debug authentication failures from log data
@debugging-agent Trace transaction flow through distributed logs

# Performance issues
@debugging-agent Profile CPU and memory usage in Go applications
@debugging-anent Debug slow database queries and connection issues
@debugging-agent Identify Redis performance bottlenecks

# Network and connectivity
@debugging-agent Debug inter-service communication failures
@debugging-agent Fix WebSocket connection and real-time issues
@debugging-anent Resolve API gateway routing and load balancing problems

# Complex issues
@debugging-anent Conduct root cause analysis for system failures
@debugging-agent Debug distributed system synchronization issues
@debugging-anent Analyze cascading failure patterns in microservices
```

## Debugging Best Practices Checklist

### Investigation Process
- [ ] Start with clear problem definition
- [ ] Gather evidence systematically
- [ ] Formulate hypotheses before testing
- [ ] Document all findings and steps
- [ ] Verify fix with comprehensive testing
- [ ] Update documentation and runbooks

### Evidence Collection
- [ ] Collect logs from all relevant services
- [ ] Gather system metrics and performance data
- [ ] Capture configuration states
- [ ] Document timeline of events
- [ ] Preserve error conditions for analysis
- [ ] Correlate data from multiple sources

### Analysis Techniques
- [ ] Use structured debugging methodology
- [ ] Apply Occam's razor (simplest explanation first)
- [ ] Consider system interactions and dependencies
- [ ] Look for patterns and correlations
- [ ] Validate hypotheses with data
- [ ] Use appropriate debugging tools

### Communication and Documentation
- [ ] Document all debugging steps and findings
- [ ] Share root cause analysis with team
- [ ] Update runbooks with new findings
- [ ] Create post-mortem reports for incidents
- [ ] Establish preventive measures
- [ ] Share knowledge and best practices

I'm here to help you navigate the most complex debugging challenges in your Blytz Live Auction platform. From systematic log analysis to root cause identification, I'll help you quickly understand and resolve issues across your entire system stack.